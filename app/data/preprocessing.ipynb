{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598820659806",
   "display_name": "Python 3.7.4 64-bit ('3.7.4': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This jupyter notebook helps to preprocess the data so that it can be used by the flask app.\n",
    "\n",
    "Input: Scientific publication\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Create directories\n",
    "1. Manually add paper and delete the part which should not be annotated.\n",
    "1. Outomatically split up paper in paragraphs of 200 words and save each separately as a txt file.\n",
    "\n",
    "Output: txt files containing 200-words-paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create directories and add paper.\n",
    "\n",
    "Create a new directory to create a new batch of hits (example name: 'batch1_argumentAnnotation'). Inside this directory, create another directory called 'entire_paper'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import ntpath\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect the current working directory and print it\n",
    "current_path = os.getcwd()\n",
    "#print (\"The current working directory is %s\" % current_path)\n",
    "batch_directory_name = 'batch1_argumentComponents'\n",
    "# define the name of the directory to be created\n",
    "entire_paper_path = os.path.join(current_path, batch_directory_name, \"entire_paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try to create the directories\n",
    "try:\n",
    "    os.makedirs(entire_paper_path)\n",
    "except OSError:\n",
    "    print(\"Creation of the directory %s failed\" % entire_paper_path)\n",
    "else:\n",
    "    print(\"Successfully created the directory %s \" % entire_paper_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Specify which part of the paper should be annotated.\n",
    "Add the entire paper as a txt file to the folder /batch1/entire_paper.\n",
    "Define which spans should not be annotated by the MTurk workers (such as for example: title, authors, abstract, keywords, references). Add the indices of the start- and the end-characted as tuples to the 'excluded_spans' list.\n",
    "\n",
    "In order to know the exact character indices you can copy the span (always from the beginning of the paper) and paste it to the 'to_be_excluded_span' string. This will tell you the length of the string which corresponds to index (with regard to the entire paper) of the last character of the string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_be_excluded_span = \"\"\"<paste text here to count characters>\"\"\"\n",
    "\n",
    "print(\"The pasted text span consistes of \" + str(len(list(to_be_excluded_span))) + \" characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define which spans should be excluded from the txt-file. Add the indices of the start- and the end-characted as tuples to the list for all spans to be excluded.\n",
    "# make sure that the spans are sorted, not overlapping, not out of bounds with regard to the entire paper length\n",
    "\n",
    "excluded_spans = [\n",
    "    (0, 2739)\n",
    "]\n",
    "#   (56252, 62243)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Outomatically split up paper in paragraphs of 200 words and save each separately as a txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tokens_from_string(offset, text, paragraph_nr_of_characters):\n",
    "    # Has offset (indices of the start character of the given text) and text as input and creates a dict which containes a dictionary for each token indicating the original start and end indices of the token.\n",
    "    tokens = {}\n",
    "    \n",
    "    # get the end indices of all sentences\n",
    "    sentences_ends = [end for start, end in list(PunktSentenceTokenizer().span_tokenize(text))]\n",
    "\n",
    "    # tokenize the given text and transform it to a list. The TreebankWordTokenizer does not discard the punctuation, therefore, we do it manaully.\n",
    "    text_tokenized = list(TreebankWordTokenizer().span_tokenize(text))\n",
    "\n",
    "    text_tokenized_discarded_punctuation = []\n",
    "    for start, end in text_tokenized:\n",
    "        # loop trough all tokens\n",
    "        if end in sentences_ends and text[end-1] == \".\" and (end-start > 1):\n",
    "            # if a token is the end of the sentence and contains a punctuation which is not yet discarded, split up token to two: one for the punctuation and one for the rest\n",
    "            text_tokenized_discarded_punctuation.append((start, end-1))\n",
    "            text_tokenized_discarded_punctuation.append((end-1, end))\n",
    "        else:\n",
    "            text_tokenized_discarded_punctuation.append((start, end))\n",
    "\n",
    "\n",
    "    # loop trough all tokens and for each create the dict before appending it to the tokens dict which will be returned by this function\n",
    "    token_id = 0\n",
    "    for start, end in text_tokenized_discarded_punctuation:\n",
    "        token_dict = {\n",
    "            \"id\": token_id,\n",
    "            \"token\": text[start:end],\n",
    "            \"start\": offset + start,\n",
    "            \"end\": offset + end,\n",
    "        }\n",
    "        tokens[token_id] = token_dict\n",
    "        token_id += 1\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def split_text_in_paragraphs(entire_paper_name, text, excluded_spans, min_paragraph_size):\n",
    "    \"\"\" Splits up a text into paragraphs containing at least n words and makes sure that text is not split in the middle of a sentence.\n",
    "    This method uses the split() function which splits up a text into words (delimiter=''). Alternatively the nltk function word_tokenize() could be used. However, the word_tokenize() finds words and punctuation in a string which is why it splits up text in more fragments than the split() method.\n",
    "    \"\"\"\n",
    "\n",
    "    entire_paper_all_characters = list(text)\n",
    "    entire_paper_all_words = text.split()\n",
    "    entire_paper_all_sentences = list(PunktSentenceTokenizer().span_tokenize(text))\n",
    "    paragraphs = {\n",
    "        \"entire_paper_name\": entire_paper_name,\n",
    "        \"entire_paper_nr_of_sentences\": len(entire_paper_all_sentences),\n",
    "        \"entire_paper_nr_of_words\": len(entire_paper_all_words),\n",
    "        \"entire_paper_nr_of_characters\": len(entire_paper_all_characters),\n",
    "        \"entire_paper_nr_of_characters_excluded\": sum([(end-start) for start, end in excluded_spans]),\n",
    "        \"excluded_spans\": excluded_spans,\n",
    "        \"text_to_annotate\": {}\n",
    "    }\n",
    "\n",
    "\n",
    "    text_spans = []\n",
    "\n",
    "    end_index_previous = 0\n",
    "    \n",
    "    if len(excluded_spans) > 0:\n",
    "        for start_index, end_index in excluded_spans:\n",
    "            if end_index_previous < start_index:\n",
    "                text_spans.append({\"start\": end_index_previous, \"end\": start_index, \"excluded\": False})\n",
    "            \n",
    "            text_spans.append({\"start\": start_index, \"end\": end_index, \"excluded\": True})\n",
    "            \n",
    "            end_index_previous = end_index\n",
    "\n",
    "    if end_index_previous < len(entire_paper_all_characters):\n",
    "        text_spans.append({\"start\": end_index_previous, \"end\": len(entire_paper_all_characters), \"excluded\": False})\n",
    "\n",
    "\n",
    "    \n",
    "    paragraph_counter = 0\n",
    "\n",
    "\n",
    "    # loop through all not excluded text spans\n",
    "    for span in [s for s in text_spans if s[\"excluded\"] is False]:\n",
    "        \n",
    "        paragraph_nr_of_sentences = 0\n",
    "        paragraph_start = None\n",
    "        paragraph_nr_of_words = 0\n",
    "        \n",
    "        span_offset = span[\"start\"]\n",
    "        span_text = text[span[\"start\"]:span[\"end\"]]\n",
    "        span_characters = list(span_text)\n",
    "        span_words = span_text.split()\n",
    "        span_sentences = [(start+span_offset, end+span_offset) for start, end in list(PunktSentenceTokenizer().span_tokenize(span_text))]\n",
    "\n",
    "        for sentence_start, sentence_end in span_sentences:\n",
    "            paragraph_nr_of_sentences += 1\n",
    "            # set the start-character of the paragraph if is is not set yet\n",
    "            if paragraph_start is None:\n",
    "                paragraph_start = sentence_start\n",
    "            \n",
    "\n",
    "            sentence_text = text[sentence_start:sentence_end]\n",
    "            # count the number of words contained in this sentence\n",
    "            sentence_nr_of_words = len(sentence_text.split())\n",
    "\n",
    "            sentence_nr_of_characters = sentence_end - sentence_start\n",
    "\n",
    "            paragraph_nr_of_words += sentence_nr_of_words\n",
    "\n",
    "            if (paragraph_nr_of_words >= min_paragraph_size):\n",
    "                \n",
    "            \n",
    "                paragraph_name = \"paragraph_\" + str(paragraph_counter)\n",
    "                tokens = get_tokens_from_string(paragraph_start, text[paragraph_start:sentence_end], sentence_end - paragraph_start)\n",
    "                paragraphs[\"text_to_annotate\"][paragraph_name] = {\n",
    "                    \"start\": paragraph_start,\n",
    "                    \"end\": sentence_end,\n",
    "                    \"nr_of_characters\": sentence_end - paragraph_start,\n",
    "                    \"nr_of_words\": paragraph_nr_of_words,\n",
    "                    \"nr_of_tokens\": len(tokens),\n",
    "                    \"nr_of_sentences\": paragraph_nr_of_sentences,\n",
    "                    \"tokens\": tokens\n",
    "                }\n",
    "                \n",
    "                paragraph_counter += 1\n",
    "                paragraph_nr_of_sentences = 0\n",
    "                paragraph_start = None\n",
    "                paragraph_nr_of_words = 0\n",
    "\n",
    "        if (paragraph_nr_of_words > 0):\n",
    "            # if the were one or more sentences left because they contained less than n words, join the words they contain and add them as the last paragraph.\n",
    "            #yield ' '.join(all_words[paragraph_length_previous:paragraph_length_previous+paragraph_length])\n",
    "            paragraph_name = \"paragraph_\" + str(paragraph_counter)\n",
    "            tokens = get_tokens_from_string(paragraph_start, text[paragraph_start:sentence_end], sentence_end - paragraph_start)\n",
    "            paragraphs[\"text_to_annotate\"][paragraph_name] = {\n",
    "                \"start\": paragraph_start,\n",
    "                \"end\": sentence_end,\n",
    "                \"nr_of_characters\": sentence_end - paragraph_start,\n",
    "                \"nr_of_words\": paragraph_nr_of_words,\n",
    "                \"nr_of_tokens\": len(tokens),\n",
    "                \"nr_of_sentences\": paragraph_nr_of_sentences,\n",
    "                \"tokens\": tokens\n",
    "            }\n",
    "\n",
    "            paragraph_counter += 1\n",
    "            paragraph_nr_of_sentences = 0\n",
    "            paragraph_start = None\n",
    "            paragraph_nr_of_words = 0\n",
    "    \n",
    "\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "# get list of all txt files in new entire_paper_path\n",
    "directory_content = glob.glob(os.path.join(entire_paper_path, '*.txt'))\n",
    "if len(directory_content) == 0:\n",
    "    print(\"There is no txt file in the directory %s . Please add the paper (in the txt format) to the directory, as expalined in section 2.\" % entire_paper_path)\n",
    "else:\n",
    "    #print(directory_content)\n",
    "    # loop trough all papers in the directory\n",
    "    for paper_path in directory_content:\n",
    "        paper_name = os.path.splitext(ntpath.basename(paper_path))[0]\n",
    "        with open(paper_path) as paper:\n",
    "            text = paper.read()\n",
    "\n",
    "            min_paragraph_size = 200\n",
    "            paragraphs = split_text_in_paragraphs(paper_name, text, excluded_spans, min_paragraph_size)\n",
    "            \n",
    "\n",
    "            paragraph_counter = 0\n",
    "            shortes_paragraph_len = None\n",
    "            longest_paragraph_len = None\n",
    "            paragraph_with_least_sentences = None\n",
    "            paragraph_with_most_sentences = None\n",
    "\n",
    "\n",
    "            for paragraph_name, paragraph_dict in paragraphs[\"text_to_annotate\"].items():\n",
    "                paragraph_nr_of_words = paragraph_dict[\"nr_of_words\"]\n",
    "                paragraph_nr_of_sentences = paragraph_dict[\"nr_of_sentences\"]\n",
    "                if shortes_paragraph_len is None or paragraph_nr_of_words < shortes_paragraph_len:\n",
    "                    shortes_paragraph_len = paragraph_nr_of_words\n",
    "                if longest_paragraph_len is None or paragraph_nr_of_words > longest_paragraph_len:\n",
    "                    longest_paragraph_len = paragraph_nr_of_words\n",
    "                if paragraph_with_least_sentences is None or paragraph_nr_of_sentences < paragraph_with_least_sentences:\n",
    "                    paragraph_with_least_sentences = paragraph_nr_of_sentences\n",
    "                if paragraph_with_most_sentences is None or paragraph_nr_of_sentences > paragraph_with_most_sentences:\n",
    "                    paragraph_with_most_sentences = paragraph_nr_of_sentences\n",
    "                \n",
    "            new_filename = paper_name + \"_ToBeAnnotated\" + \".json\"\n",
    "            new_filepath = os.path.join(current_path, batch_directory_name, new_filename)\n",
    "            with open(new_filepath, 'w') as new_json_file:\n",
    "                new_json_file.write(json.dumps(paragraphs, indent=2))\n",
    "            \n",
    "\n",
    "            print(\"The text contains %s characters in total.\" % paragraphs[\"entire_paper_nr_of_characters\"])\n",
    "            print(\"The text contains %s words in total.\" % paragraphs[\"entire_paper_nr_of_words\"])\n",
    "            print(\"The text contains %s sentences in total.\" % paragraphs[\"entire_paper_nr_of_sentences\"])\n",
    "            print(\"The text was split up in %s paragraphs.\" % len(paragraphs[\"text_to_annotate\"]))\n",
    "            print(\"Paragraphs contain %s characters on average.\" % (paragraphs[\"entire_paper_nr_of_characters\"]/len(paragraphs[\"text_to_annotate\"])))\n",
    "            print(\"Paragraphs contain %s words on average.\" % (paragraphs[\"entire_paper_nr_of_words\"]/len(paragraphs[\"text_to_annotate\"])))\n",
    "            print(\"Paragraphs contain %s sentences on average.\" % (paragraphs[\"entire_paper_nr_of_sentences\"]/len(paragraphs)))\n",
    "            print(\"The shortest paragraph contains %s words.\" % shortes_paragraph_len)\n",
    "            print(\"The longest paragraph contains %s words.\" % longest_paragraph_len)\n",
    "            print(\"The paragraph with the least sentences contains %s sentences.\" % paragraph_with_least_sentences)\n",
    "            print(\"The paragraph with the moset sentences contains %s sentences.\" % paragraph_with_most_sentences)\n",
    "            print(\"The preprocessing result containing all %s paragraphs was saved as a json file (filename=%s) in the folder %s.\" % (len(paragraphs[\"text_to_annotate\"]), new_filename, batch_directory_name))"
   ]
  }
 ]
}